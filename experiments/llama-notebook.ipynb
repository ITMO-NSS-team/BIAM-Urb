{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "\n",
    "import json\n",
    "import geojson\n",
    "from typing import Any, Dict, Tuple\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import torch\n",
    "from transformers import (AutoTokenizer,\n",
    "                          pipeline,\n",
    "                          AutoModelForCausalLM,\n",
    "                          BitsAndBytesConfig,\n",
    "                          AutoConfig, GenerationConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = Path('').resolve().parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(ROOT / 'config.env')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'meta-llama/Meta-Llama-3-8B-Instruct' #\"meta-llama/Meta-Llama-3-8B\"\n",
    "HF_TOKEN = os.environ.get('HF_TOKEN')\n",
    "SAVE_FOLDER = Path(ROOT / 'llama_answers_dataset_en_instruction').resolve()\n",
    "SAVE_FOLDER.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building area should be rounded to 3 decimal places.\n",
    "# If the answer means using numerical characteristics of the building, round all numbers in the answer.\n",
    "# Number of residents in the buildings should always be an integer in answer.\n",
    "# Если в ответе идет речь про численные характеристики здания, округли все цифры в ответе по математическии, например 115.4 станет 115, 234.8 станет 235."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERICAL_DATA = ['building_area', 'living_area', 'storeys_count', 'resident_number', 'population_balanced', 'lift_count', 'building_year']\n",
    "\n",
    "\n",
    "def preprocess_context(data: Dict) -> Dict:\n",
    "    \"\"\"Preprocess context data by rounding all digits in manually set fields.\"\"\"\n",
    "    preprocessed = deepcopy(data)\n",
    "    for chunk in preprocessed['features']:\n",
    "        properties = chunk['properties']\n",
    "        for num_col in NUMERICAL_DATA:\n",
    "            if properties.get(num_col) is not None:\n",
    "                properties[num_col] = round(float(properties[num_col]))\n",
    "    return preprocessed\n",
    "        \n",
    "\n",
    "\n",
    "def get_prompt(question: str, context: Dict, *args, **kwargs) -> str:\n",
    "    \"\"\"Function for intialization of LLAMA3 prompt template.\"\"\"\n",
    "    default = '''You are a smart AI assistant, developed to help users with their questions.'''\n",
    "    default_rules = f'''Answer the question following rules below. For answer you must use provided by user context. Your answer should be in Russian language.\n",
    "    Rules:\n",
    "    The answer should have three sentences maximum. The answer should be short, without any explanation, and redundant or unrelated information.\n",
    "    Add a unit of measurement to an answer.\n",
    "    If there are several organizations in the building, all of them should be mentioned in the answer.\n",
    "    The building's address (street, house number, building) in the user's question should exactly match a building address from the context.\n",
    "    For answer you should take only that infromation from context, which exactly match a building address (street, house number, building) from the user's question.\n",
    "    If provided by user context for a given address has \"null\" or \"None\" for the property, it means the data about this property of the building is absent.\n",
    "    In questions about building failure, 0 in the context's corresponding field means \"no\", and 1 - means \"yes\".\n",
    "    If data for an answer is absent, answer that data was not provided or absent and mention for what field there was no data.\n",
    "    If you do not know how to answer the questions, say so.'''\n",
    "    system_prompt = kwargs.get('system_prompt', default)\n",
    "    rules = kwargs.get('additional_rules', default_rules)\n",
    "    template = f\"\"\"\n",
    "            <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
    "            {system_prompt} {rules}<|eot_id|>\n",
    "            <|start_header_id|>user<|end_header_id|>\n",
    "            Контекст :{context} Вопрос: {question}<|eot_id|>\n",
    "            <|start_header_id|>assistant<|end_header_id|>\n",
    "            \"\"\"\n",
    "    return template\n",
    "\n",
    "\n",
    "def save_answer_as_json(answer: Dict) -> None:\n",
    "    total_files_in_folder = len(glob.glob1(str(SAVE_FOLDER), '*.json'))\n",
    "    f_pref = 'llama_ans'\n",
    "    with open((SAVE_FOLDER/f'{f_pref}_{total_files_in_folder + 1}.json').resolve(), 'w', encoding='utf-8') as pth:\n",
    "        json.dump(answer, pth)\n",
    "\n",
    "\n",
    "def get_query(idx: int) -> Tuple[Dict, Dict]:\n",
    "    \"\"\"Load set of queries and context to them.\"\"\"\n",
    "    with open(Path(ROOT, 'data', 'datasets', f'data_{idx}.json')) as json_data:\n",
    "        questions = json.load(json_data)\n",
    "    with open(Path(ROOT, 'data', 'buildings', f'buildings_part_{idx}.geojson')) as buildings_data:\n",
    "        manual_context = geojson.load(buildings_data)\n",
    "    return questions, manual_context\n",
    "\n",
    "\n",
    "def multi_ans(model: Any, amount: int = 10, **kwargs) -> None:\n",
    "    \"\"\"\n",
    "    Get multiple answers from given model.\n",
    "    This function loads contexts from several files\n",
    "    \"\"\"\n",
    "    generation_temperature = kwargs.get('temperature', .5)\n",
    "    for i in range(amount):\n",
    "        queries, context = get_query(i)\n",
    "        total_questions = list(queries.keys())\n",
    "        for q_id in range(len(total_questions) // 5):\n",
    "            # Pick one query from the list\n",
    "            question_response_pair: Dict= queries[total_questions[q_id]]            \n",
    "            query, response = question_response_pair['query'], question_response_pair['response']\n",
    "\n",
    "            #Form a prompt from query and context\n",
    "            prompt = get_prompt(question=query, context=preprocess_context(context))\n",
    "            answer = model(prompt, temperature=generation_temperature)\n",
    "\n",
    "            json_ans = {'query': query, \n",
    "                        'llama_answer': answer[0][\"generated_text\"].split(\"<|end_header_id|>\")[-1], \n",
    "                        'ideal_ans': response,\n",
    "                        'chunk': i,\n",
    "                        'question_number': total_questions[q_id]}\n",
    "            save_answer_as_json(json_ans)\n",
    "    print(f'Answers have been saved to {SAVE_FOLDER}. Amount: {len(glob.glob1(str(SAVE_FOLDER), \"*.json\"))}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/urbanistic/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d99037ae1c48678fd3fe4eabcce2fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/654 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_8bit=True, llm_int8_enable_fp32_cpu_offload=True)\n",
    "model_config = AutoConfig.from_pretrained(MODEL_NAME, trust_remote_code=True, max_new_tokens=12000, force_download=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afd77ffb15f7447684a3a0079934af5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is ready.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    # quantization_config=quantization_config, ## Uncomment, if quantization is required\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    token=HF_TOKEN,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.generation_config.pad_token_ids = tokenizer.pad_token_id\n",
    "model.eval()\n",
    "print('Model is ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = pipeline(\n",
    "    'text-generation', \n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    "    max_length=12000,\n",
    "    device_map='auto',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MULTI ans testing\n",
    "# multi_ans(pipeline, amount=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions, manual_context = get_query(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question_id: 51_3\n",
      "Question: Какой тип проекта застройки дома по адресу \"Санкт-Петербург, Шепетовская, 3\"?\n"
     ]
    }
   ],
   "source": [
    "# File in question has multiple varinats of questions, we have to pick one\n",
    "question_id = list(questions.keys())[25]\n",
    "question_response_pair = questions[question_id]\n",
    "manual_question = question_response_pair['query']\n",
    "target = question_response_pair['response']\n",
    "\n",
    "print(f'Question_id: {question_id}')\n",
    "print(f'Question: {manual_question}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = get_prompt(question=manual_question, context=preprocess_context(manual_context))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вопрос: Какой тип проекта застройки дома по адресу \"Санкт-Петербург, Шепетовская, 3\"?\n",
      "Верный ответ: Для дома по адресу \"Санкт-Петербург, Шепетовская, 3\" данная информация отсутствует\n",
      "Ответ: \n",
      "             Тип проекта застройки дома по адресу \"Санкт-Петербург, Шепетовская, 3\" - \"1-528\".\n"
     ]
    }
   ],
   "source": [
    "answer = pipeline(query, temperature=.2)\n",
    "\n",
    "print(f'Вопрос: {manual_question}')\n",
    "print(f'Верный ответ: {target}')\n",
    "print(f'Ответ: {answer[0][\"generated_text\"].split(\"<|end_header_id|>\")[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "urbanistic",
   "language": "python",
   "name": "urbanistic"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
